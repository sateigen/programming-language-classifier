{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polyglot\n",
    "This project was designed to deepen our understanding of feature extraction, classification, and building a robust classifier. Specifically, the classifier that this project uses can take snippets of code and guesses the programming language of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Files are read in and set as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_extensions = {'gcc': 'c', 'c': 'c', 'csharp': 'c#', 'sbcl': 'common lisp',\n",
    "                   'clojure': 'clojure', 'hs': 'haskell', 'java': 'java',\n",
    "                   'javascript': 'javascript', 'ocaml': 'ocaml', 'perl':\n",
    "                   'perl', 'hack': 'php', 'php': 'php', 'python3': 'python',\n",
    "                   'jruby': 'ruby', 'yarv': 'ruby', 'scala': 'scala',\n",
    "                   'racket': 'scheme', 'tcl': 'tcl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_program_files(file_locations):\n",
    "    files = glob.glob(file_locations, recursive=True)\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(file, encoding='latin_1') as f:\n",
    "            texts.append(f.read())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for key, value in file_extensions.items():\n",
    "    X = read_program_files('benchmarksgame/bench/**/*.{}'.format(key))\n",
    "    X_train += X\n",
    "    y_train += (len(X) * [value])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A counter was used to ensure there were not too many files of any one language used to train the classifier.  Ruby and JavaScript stand out as  needing fewer and more files, respectively, to improve the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'c': 59,\n",
       "         'c#': 41,\n",
       "         'clojure': 38,\n",
       "         'common lisp': 34,\n",
       "         'haskell': 52,\n",
       "         'java': 51,\n",
       "         'javascript': 25,\n",
       "         'ocaml': 35,\n",
       "         'perl': 34,\n",
       "         'php': 55,\n",
       "         'python': 36,\n",
       "         'ruby': 73,\n",
       "         'scala': 43,\n",
       "         'scheme': 29,\n",
       "         'tcl': 52})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline created to fit and transform the data through the CountVectorizer estimator before transforming the data with the MulitnomialNB estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pip = Pipeline([\n",
    "        ('vect', CountVectorizer(analyzer='word', token_pattern=r'\\w{2,}|\\s{2,}|[^\\w\\d\\s]')),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='\\\\w{2,}|\\\\s{2,}|[^\\\\w\\\\d\\\\s]',\n",
       "        tokenizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The mean accuracy of the training files and labels provided to train the classifier.\n",
    "This score should be close to 1.0, so the score of 0.98 for this test data shows that the classifier is ready to use on unseen files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98325722983257224"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Files are read in and set as testing data and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for num in range(1, 33):\n",
    "    X = read_program_files('test/{}'.format(num))\n",
    "    X_test += X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = []\n",
    "with open('test.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        y_test.append(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The mean accuracy of the test files and labels provided.\n",
    "A decent score for testing data should be around 0.8.  This score is slightly lower than would be preferred;  however, it is most likely due to the overlapping similarities between many of the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78125"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first list of languages is what language the classifier is predicting each file is written in. The second list is the programming language each file was actually written in.  Comparing the two, it is clear that this classifier has the most difficulty distinguishing java files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clojure' 'clojure' 'clojure' 'clojure' 'python' 'python' 'ruby' 'python'\n",
      " 'javascript' 'javascript' 'scala' 'scala' 'ruby' 'ruby' 'ruby' 'haskell'\n",
      " 'haskell' 'tcl' 'scheme' 'scheme' 'scheme' 'c' 'c' 'scala' 'scala' 'tcl'\n",
      " 'tcl' 'c' 'php' 'php' 'ocaml' 'ocaml']\n",
      "['clojure', 'clojure', 'clojure', 'clojure', 'python', 'python', 'python', 'python', 'javascript', 'javascript', 'javascript', 'javascript', 'ruby', 'ruby', 'ruby', 'haskell', 'haskell', 'haskell', 'scheme', 'scheme', 'scheme', 'java', 'java', 'scala', 'scala', 'tcl', 'tcl', 'php', 'php', 'php', 'ocaml', 'ocaml']\n"
     ]
    }
   ],
   "source": [
    "print(pip.predict(X_test))\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
